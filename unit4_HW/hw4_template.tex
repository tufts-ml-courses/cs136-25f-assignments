\documentclass[10pt]{article}

%%% Doc layout
\usepackage{fullpage} 
\usepackage{booktabs}       % professional-quality tables
\usepackage{microtype}      % microtypography
\usepackage{parskip}
\usepackage{times}

%% Hyperlinks always black, no weird boxes
\usepackage[hyphens]{url}
\usepackage[colorlinks=true,allcolors=black,pdfborder={0 0 0}]{hyperref}

%%% Math typesetting
\usepackage{amsmath,amssymb}
\usepackage{pythonhighlight} 
%%% Write out problem statements in blue, solutions in black
\usepackage{xcolor}
\newcommand{\officialdirections}[1]{{\color{purple} #1}}

%%% Avoid automatic section numbers (we'll provide our own)
\setcounter{secnumdepth}{0}


%% --------------
%% Header
%% --------------
\usepackage{fancyhdr}
\fancyhf{}
\setlength{\headheight}{15pt}
\fancyhead[C]{\ifnum\value{page}=1 Tufts CS 136 - 2025f - HW4 Submission \else \fi}
\fancyfoot[C]{\thepage} % page number
\renewcommand\headrulewidth{0pt}
\pagestyle{fancy}

%% --------------
%% Begin Document
%% --------------
\begin{document}

~\\ %% add vertical space
{\Large{\bf Student Name: TODO}}

~\\ %% add vertical space
{\bf Collaboration Statement:}

Total hours spent: TODO

I consulted the following human, textbook, or AI resources:
\begin{itemize}
\item TODO
\item TODO
\item $\ldots$	
\end{itemize}

By turning this document in, I attest that I have followed the 
\href{https://www.cs.tufts.edu/cs/136/2025f/index.html#collaboration}{[CS 136 Collaboration Policy]}. All solutions represent my own work. No solution text in this document was directly provided by other humans or artificial agents. 

\tableofcontents

\newpage

\officialdirections{
\subsection*{1a: Problem Statement}
Find the optimal one-hot assignment vectors $r^1$ for all $N=7$ examples, given the initial cluster locations $\mu^0$. Report the value of the cost function $J(x, r^1, \mu^0)$.
}

\subsection{1a: Solution}
TODO FILL IN TABLE

\begin{tabular}{p{5cm} | p{5cm} | r}
$\mu^0$ & $r^1$ & $J(x_{1:N}, r^1, \mu^0)$
\\
\midrule
\begin{verbatim}
[[-3.  -2. ]
 [ 1.5  3. ]
 [ 2.   2. ]]
\end{verbatim}
&	
\begin{verbatim}
[[? ? ?]
 [? ? ?]
 [? ? ?]
 [? ? ?]
 [? ? ?]
 [? ? ?]
 [? ? ?]]
\end{verbatim}
&
 ??.?????
\end{tabular}

\officialdirections{
\subsection*{1b: Problem Statement}
 Find the optimal cluster locations $\mu^1$ for all K=3 clusters, using the optimal assignments $r^1$ you found in 1a. Report the value of the cost function $J(x, r^1, \mu^1)$.}

\subsection{1b: Solution}
TODO FILL IN TABLE

\begin{tabular}{p{5cm} | p{5cm} | r}
$\mu^1$ & $r^1$ & $J(x_{1:N}, r^1, \mu^1)$
\\
\midrule
\begin{verbatim}
[[ ?.???  ?.???]
 [ ?.???  ?.???]
 [ ?.???  ?.???]]
\end{verbatim}
&	
\begin{verbatim}
[[? ? ?]
 [? ? ?]
 [? ? ?]
 [? ? ?]
 [? ? ?]
 [? ? ?]
 [? ? ?]]
\end{verbatim}
&
 ??.?????
\end{tabular}

\officialdirections{
\subsection*{1c: Problem Statement}
How does the cost function change from 1a to 1b? Does this align with known guarantees for the k-means algorithm?
}
\subsection{1c: Solution}

TODO

\officialdirections{
\subsection*{1d: Problem Statement}
Execute another iteration of k-means: use the $\mu^1$ from 1b to get new assignments $r^2$, and then use those assignments to update the locations $\mu^2$. 

Inspecting the values of $r^2$ and $\mu^2$, what interesting phenomenon do you see happening regarding the second cluster? Instead of what deterministically happens, how else could you set cluster 2's location inside $\mu^2$ to better fulfill the goals of K-means (find K clusters that reduce cost the most)?
}
\subsection{1d: Solution}

TODO


\officialdirections{
\subsection*{2a: Problem Statement}
Show with math that using the parameter settings defined above, the general formula for $\gamma_{nk}$ will simplify to the following (inspired by PRML Eq. 9.42):

\begin{align}
\gamma_{nk} = \frac
	{ \text{exp}( - \frac{1}{2\epsilon} (x_n - \mu_k)^T(x_n - \mu_k) )}
	{ \sum_{j=1}^K \text{exp}( - \frac{1}{2\epsilon} (x_n - \mu_j)^T (x_n - \mu_j) )}
\end{align}

}

\subsection{2a: Solution}
TODO

\officialdirections{
\subsection*{2b: Problem Statement}
What will happen to the vector $\gamma_n$ as $\epsilon \rightarrow 0$? How is this related to K-means?
}

\subsection{2b: Solution}

TODO

\newpage 
\officialdirections{
\subsection*{3a: Problem Statement}
Given: $m = \mathbb{E}_{p^{\text{mix}(x)}}[x]$.
Prove that the covariance of vector $x$ is:
\begin{align}
\text{Cov}_{p^{\text{mix}}(x)}[x] = \sum_{k=1}^K \pi_k (\Sigma_k + \mu_k \mu_k^T ) - m m^T
\end{align}
}
\subsection{3a: Solution}
TODO

\newpage
\officialdirections{
\subsection*{4a (OPTIONAL): Problem Statement}
Consider any two Categorical distributions $q(z)$ and $p(z)$ that assign positive probabilities over the same size-$K$ sample space. Show that their KL divergence is non-negative.  That is, show that
\begin{align}
\text{KL}\left( \text{CatPMF}(z | \mathbf{r}) || \text{CatPMF}(z | \mathbf{\pi}) \right) &\geq 0
\end{align}

when $\mathbf{r} \in \Delta^K_+$ and $\mathbf{\pi} \in \Delta^K_+$.

}

\subsection{4a: Solution}

TODO


\end{document}
