\documentclass[10pt]{article}

%%% Doc layout
\usepackage{fullpage} 
\usepackage{booktabs}       % professional-quality tables
\usepackage{microtype}      % microtypography
\usepackage{parskip}
\usepackage{times}

%% Hyperlinks always black, no weird boxes
\usepackage[hyphens]{url}
\usepackage[colorlinks=true,allcolors=black,pdfborder={0 0 0}]{hyperref}

%%% Math typesetting
\usepackage{amsmath,amssymb}

%%% Write out problem statements in blue, solutions in black
\usepackage{xcolor}
\newcommand{\officialdirections}[1]{{\color{purple} #1}}

%%% Avoid automatic section numbers (we'll provide our own)
\setcounter{secnumdepth}{0}

%% --------------
%% Header
%% --------------
\usepackage{fancyhdr}
\fancyhf{}
\setlength{\headheight}{15pt}
\fancyhead[C]{\ifnum\value{page}=1 Tufts CS 136 - 2025f - HW1 Submission \else \fi}
\fancyfoot[C]{\thepage} % page number
\renewcommand\headrulewidth{0pt}
\pagestyle{fancy}

%% --------------
%% Begin Document
%% --------------
\begin{document}

~\\ %% add vertical space
{\Large{\bf Student Name: TODO}}

~\\ %% add vertical space
{\bf Collaboration Statement:}

Total hours spent: TODO

I consulted the following human, textbook, or AI resources:
\begin{itemize}
\item TODO
\item TODO
\item $\ldots$	
\end{itemize}

By turning this document in, I attest that I have followed the 
\href{https://www.cs.tufts.edu/cs/136/2025f/index.html#collaboration}{[CS 136 Collaboration Policy]}. All solutions represent my own work. No solution text in this document was directly provided by other humans or artificial agents. 

\tableofcontents

\newpage

\officialdirections{
\subsection*{1a: Problem Statement}

Let $\rho \in (0.0, 1.0)$ be a Beta-distributed random variable: $p \sim \text{Beta}(a, b)$. 

Show that $\mathbb{E}[ \rho ] = \frac{a}{a + b}$.

\textbf{Hint:} You can use these identities, which hold for all $a > 0$ and $b > 0$:

\begin{align}
\Gamma(a) &= \int_{t=0}^{\infty} e^{-t} t^{a-1} dt
\\
\Gamma(a+1) &= a \Gamma(a)
\\
\int_{0}^1 \rho^{a-1} (1-\rho)^{b-1} d\rho &= \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}
\end{align}
}

\subsection{1a: Solution}
TODO YOUR SOLUTION HERE. You do not need to do anything too complicated to show your justification alongside your math. You can simply alternate between explanation and \texttt{\textbackslash align} statements, as below:
\\

TODO: Delete this text and replace with your solution. It is provided as a demonstration of one possible way to format math explanations in \LaTeX \\
We begin with applying product rule to the joint distribution:
\begin{align*}
    p(A,B) &= p(A|B)p(B)
\end{align*}
We divide both sides by $p(B)$
\begin{align*}
    \frac{p(A,B)}{p(B)} &= p(A|B)
\end{align*}
And we again apply the product rule to the numerator of the left-hand side to obtain Bayes' rule:
\begin{align*}
    \frac{p(B|A)p(A)}{p(B)} &= p(A|B)
\end{align*}

\newpage 

\officialdirections{
\subsection*{1b: Problem Statement}

Let $\mu$ be a Dirichlet-distributed random variable: $\mu \sim \text{Dir}(a_1, \ldots a_V)$. 

Show that $\mathbb{E}[ \mu_w ] = \frac{a_w}{\sum_{v=1}^V a_v}$, for any integer $w$ that indexes a vocabulary word.

\textbf{Hint:} You can use the identity:
\begin{align}
\int \mu_1^{a_1-1} \mu_2^{a_2 - 1} \ldots \mu_V^{a_V-1} d\mu
 &= \frac
 	{\prod_{v=1}^V \Gamma(a_v)}
 	{\Gamma(a_1 + a_2 \ldots + a_V)}
\end{align}
}

\subsection{1b: Solution}
TODO YOUR SOLUTION HERE

\newpage
\officialdirections{
\subsection*{2a: Problem Statement}

Show that the likelihood of all $N$ observed words can be written as:
\begin{align}
p(X_1 = x_1, X_2 = x_2, \ldots, X_N = x_N | \mu) = \prod_{v=1}^V \mu_v^{n_v}
\end{align}
where $n_v$ is the count of vocabulary term $v$ across all $N$ observed words.

\textbf{Hint:}
It may be helpful to recall the definition of the Categorical PMF using Iverson bracket indicator notation:
\begin{align}
p(X_n = x_n | \mu) = \prod_{v=1}^V \mu_v^{[x_n = v]}
\end{align}
Using this bracket notation,  the count $n_v$ can mathematically defined as:  $n_v =  \sum_{n=1}^N [x_n = v]$.
}

\subsection{2a: Solution}
TODO YOUR SOLUTION HERE

\newpage
\officialdirections{
\subsection*{2b: Problem Statement}

Derive the next-word posterior predictive, after integrating away parameter $\mu$.

That is, show that after seeing the $N$ training words, the probability of the next word $X_*$ being vocabulary word $v$ is:
\begin{align}
p( X_* = v | X_1 = x_1 \ldots X_N = x_N)
	&= \int p( X_* = v, \mu | X_1 = x_1 \ldots X_N = x_N) d\mu
\notag \\
	&= \frac{n_v + \alpha}{N + V\alpha}
\end{align}
\textbf{Hint:} You will use the expectation of a Dirichlet-distributed random variable that we proved in 1b
}
\subsection{2b: Solution}

TODO YOUR SOLUTION HERE




\newpage
\officialdirections{
\subsection*{2c: Problem Statement}
Derive the marginal likelihood of observed training data, after integrating away the parameter $\mu$.

That is, show that the marginal probability of the observed $N$ training words has the following closed-form expression:
\begin{align}
p( X_1 = x_1 \ldots X_N = x_N) 
	&= \int p( X_1 = x_1, \ldots X_N = x_N, \mu ) d\mu
	\\
	&= \frac
	{ \Gamma(V \alpha)      \prod_{v=1}^V \Gamma( n_v + \alpha ) }
	{ \Gamma(N + V \alpha ) \prod_{v=1}^V \Gamma(\alpha)         }
\end{align}
}


\subsection{2c: Solution}
TODO YOUR SOLUTION HERE


\end{document}
